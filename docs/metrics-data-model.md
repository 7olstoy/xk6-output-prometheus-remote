# k6 Time Series

We would migrate to a time series data model for getting the benefits of aggregated data so just allocating samples one time and pass around the aggregated time series value (or values - e.g. in case of Trend). The Outputs will switch from fetching the buffered samples to fetching a snapshot of the time series (the same concept adopted by Prometheus' scraping operation).

## Data model

#### Metrics and Series

* Metric (name, type)
* TimeSeries (metric, tagset)
* Sample (metric, tagset, timestamp, value)
* TimeSeries Sample (timeseries ref, timestamp, value)

```go
// This is the current k6 Metric polished from the Thresholds stuff.
type Metric struct {
	Name     string     `json:"name"`
	Type     MetricType `json:"type"`
	Contains ValueType  `json:"contains"`
}

type TimeSeries struct {
    ID uin64 // hash(metric+tags)
	MetricName string
	Tags   TagSet

	Sink Sink // Counter, Guage, Rate, Trend (SparseHistogram in the future)
}

type Sink interface {
    AddPoint(v float64)
}

type TSample struct { 
	TimeSeriesID string
	Timestamp      uint64
	Value          float64
}

type Sample struct {
        MetricName string
        Timestamp uint64
        Tags TagSet
        Value float64
}
```

#### Open questions:

* `TimeseriesSample` knows the time series concept and it has the reference to a time series. The Sample is raw data and there isn't yet an identified/matched time series to reference. In this way, the collecting phase doesn't  require knowledge from a time series database. It could speed up the  implementation and introduction of the first stage, where hopefully we don't touch the sample generation (e.g. all the `k6/js/modules`).
* In the case the two types (`Sample` and `TSample`) are confirmed, we need the `TagSet` in the Sample or can we just simplify it as a `[]string` or `map[string]string` and delegate it in a centralized place the `TagSet` association?
* Currently, the metric type is defined by the `metrics.Metric` but the same type handle Thresholds and Sink operation. Should the `TimeSeries` type depend on `Metric`? Or should we wait to split them in a different struct?

#### Tags (aka [k6/1831](https://github.com/grafana/k6/issues/1831))

* Tag{Key, Value}
* TagSet ([]*Tag)

#### Sink

The sinks are implemented by metric types and they keep the series values up to date:

* Counter:  a monotonic increasing value
* Gauge: the latest value
* SparseHistogram: counters per dynamic ranges (buckets) - https://grafana.com/blog/2021/11/03/how-sparse-histograms-can-improve-efficiency-precision-and-mergeability-in-prometheus-tsdb

#### Series IDs

The series IDs are a `uint64` value generated by hashing the metric name and the tags' key-value pairs.

#### Timestamp

The timestamp is a `uint64` value representing a Unix nano.

## Storage

#### Time Series database (aka tsdb)

We need to store all the time series generated from `k6` run during the execution so the other components (mostly the outputs) could query the storage for getting the value of the time series. It's expected to be in-memory and it should be concurrent-safe.

```go
type Repository interface {
    InsertSeries(TimeSeries) error
    GetSeriesByID(hash uint64) (*TimeSeries, error) 
    GetSeries() ([]*TimeSeries, error) // maybe, we should try to avoid it
}
```

#### TagMap

```go
type TagMap struct {
    m sync.Map // potentially something better
}
```

It creates a finite set of tags storing the pointer of Tags by its hash. In this way, it should reduce the used memory and the GC operations.

#### Open questions:

* Is the same structure by TagSet required? It could be useful in the case we need to compare a pair of TagSet, where we can just execute single access by hash instead of N comparisons.
* Should it be possible for any reason to fetch raw data points (not aggregated)? Like `samples := tsdb.GetSamplesBySeries(seriesID, timeStart, timeEnd)`? 

## Collect

#### Open questions:

* Should we change something in `metrics.PushIfNotDone()`?
* Do we require to identify the time series before invoking `metrics.PushIfNotDone`? So, does the sample require to reference the time series?

## metrics.Ingester

TBD

## Outputs

Outputs should interact with the `tsdb` for fetching a time series snapshot. `series := tsdb.GetSeries()`.

#### Open questions:

* Should we query by timeframe so we can filter out time series without samples and avoid pushing the time series without a changed value?

#### Aggregation

Using time series from the outputs as described is already a form of aggregation. 

#### Open questions:

* Do we require high accurate timeframe for aggregation? Currently, the aggregation window is defined by the `PeriodicFlusher` but in the high concurrent world like `k6` it could be not exactly the expected timeframe (e.g. instead to be now-1s <= t < now).

#### Concurrency

In the case, that the write endpoint for flushing data is slow and it returns the response in a greater interval than the interval set for the `PeriodicFlusher` then we could push a not optimal aggregated window where the aggregation is based on a wider range.

Is this an issue? If yes:

* In the case the Prometheus instance slows down in a limited range then we could mitigate it by adding concurrent flush. It requires adding a label per goroutine (e.g. a Worker ID ?) so we can avoid hitting out-of-order issues. 

#### Prometheus

##### Trend

[:bulb:] We could potentially mitigate the sparse histogram by making the histogram configurable by adding two options:

* UseTrendAsHistogram: where it says to the output to use the histogram for Trend.
* HistogramBuckets: it would require a map between Trend metrics' name and the expected histogram.

In the case, we couldn't directly implement the right dynamic histogram, this would help advanced users that know how the SUT behaves to use histogram instead of the 6 gauges workaround.

##### End of series markers

Is the marker required and/or useful for the output?

#### Thresholds

Thresholds should act as an output, it should work on top of a time series data layer. They get the latest value for the time series and check if the value triggers the threshold based on the defined condition.

## Known issues

* Name and URL tags: `k6` is tagging HTTP requests with the URL. It will create a high cardinality issue for the time series data model.
* We need to keep all the data for the Trend type for computing the percentiles. We plan to migrate to some form of approximation (Sparse Histogram, T-Digest, etc..)

## Roadmap proposal

1. Add a simpler implementation in the Prometheus Output extension
2.  Collect feedback and re-iterate
3. Use it directly into the k6 core (two options):
   1. We could use the new model for some parts of the core and keep the old in parallel with the new model then gradually migrate the rest.
   2. Replace the entire data model in one shot

## Acceptance criteria

- [ ] Can the new model enable the Prometheus output integration?
- [ ] Can the new model enable the cloud aggregation?
- [ ] Can the new model work with Thresholds?
- [ ] Is the memory footprint generated from the new model reduced? If not, is it acceptable?
- [ ] Is the CPUs usage generated from the new model reduced? If not, is it acceptable?
